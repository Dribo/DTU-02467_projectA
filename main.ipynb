{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Explainer Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### LINK TO WEBPAGE\n",
    "\n",
    "https://dribo.github.io/DTU-02467_ProjectB/\n",
    "\n",
    "### LINK TO GITHUB\n",
    "https://github.com/Dribo/DTU-02467_projectA\n",
    "\n",
    "\n",
    "# Motivation\n",
    "\n",
    "We get our dataset by scraping the cold war on Wikipedia. Then all references are also scraped. Each row is a wikipedia page with data: Title, Text, References, Categories\n",
    "\n",
    "We chose this particular dataset because the Cold War is an interesting subject, and we suspect that the page of choice being english, might cause bias, since the war is very polarizing depending on country and language.\n",
    "\n",
    "Our goal was to shed light on how wikipedia might contain bias, and what it might look like.\n",
    "\n",
    "# Basic Stats\n",
    "\n",
    "We chose to scrape at a depth of 1, since this scrape was already very time-consuming.\n",
    "\n",
    "When we were building the Network, we ended up filtering heavily, because we found wikipedia to be too heavily interlinked for the purposes of our Text Analysis.\n",
    "\n",
    "Our complete network has 857 nodes and 16216 edges.\n",
    "\n",
    "# Tools, theory and analysis\n",
    "\n",
    "We used networkX to represent the graph object, Louvain Detection for communities.\n",
    "\n",
    "We created our own classification using heuristics that we then used to create subgraphs. We wanted to get a good distinction between events and people on wikipedia. We found our classification to be decent by inspecting the classifications.\n",
    "\n",
    "Afterwards, we examined assortativity on the full graph and our metric. This value was 0.05. Since we are sure our classification was not that bad, we now know that people and events will not be useful for community detection, and hence we worked with a subgraph for solely 'event' from then on.\n",
    "\n",
    "We spent some time experimenting with subgraphs, because it was difficult to get a good community segmentation. We finally reached a good modularity by removing high-degree nodes. We used distributions of the data, for example node degree distributions, to aid in our choice of thresholds for the subgraph filtering.\n",
    "\n",
    "For text analysis we used the communities for TF-IDF analysis. We attempted to answer our research question about finding bias in Wikipedia by looking at what words and distinctions came to surface, and whether these were surprising or not.\n",
    "\n",
    "We decided to use the sklearn implementation of the TF-IDF vectorisation. This was done mainly to avoid having to write a less efficient code than what was already available. A downside to this was that all pre-processing was done behind the scenes and as such we relenquished some control over the exact way our text data was pre-processed. We did take a look at the result of the pre-processing and found it to be in exactly the same vein that we ourselves would have written, and therfore we decided to move forward with this approach. To sum up, the pre-processing of the text was done by sklearn using their tokenizer, where all punctuation for instance counts as token seperators, compared to only using spaces or having some abbreviation count as one token, i.e U.S. We did however decided that this was a worthwhile trade-off compared to having to write the code ourselves which in any case would be less efficient.\n",
    "\n",
    "# Discussion\n",
    "\n",
    "In general the project went well. We found that Wikipedia has a large amount of data available, and that articles are more interlinked than perhaps we intially expected. This made it necessary to be a bit creative in what heuristics we employed in order to get a subgraph that was both interesting to analyse and which also could be used to answer the questions we wanted.\n",
    "We also found that using a ready-made library in SKLearn for the TF-IDF gave us more time to actually analyse the data rather than spending the time writing code that anyways would be less efficient. It did however mean that we had less of a say in how the pre-processing was done, which obviously gave us less independence in the results. On that note, however, we found that the pre-processing actually made sense, and we didn't find any obvious deviations from how we would have done it ourselves.\n",
    "\n",
    "Some of the things we would have wished to expand on is sentiment analysis and looking at other languages. We think this would have enabled a deeper, more interesting analysis, and would have been a better way to answer the research question we intially looked at.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Options\n",
    "OPTION_PERFORM_SCRAPE = False\n",
    "OPTION_SAVE_FIG = True\n",
    "OPTION_SHOW_PLOT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from ast import literal_eval\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "from netwulf.interactive import visualize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import community\n",
    "from collections import OrderedDict\n",
    "from operator import getitem\n",
    "import importlib\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Static Variables\n",
    "LINK_WIKI_ENGLISH = \"https://en.wikipedia.org/wiki/Cold_War\"\n",
    "LINK_WIKI_GERMAN = \"https://de.wikipedia.org/wiki/Kalter_Krieg\"\n",
    "EVENT_FILTER = [\"conflicts\", \"conflict\", \"events\", \"event\", \"wars\", \"war\", \"coups\", \"coup\",\n",
    "                \"crises\", \"crisis\", \"coup d'Ã©tat\", \"history\", \"warfare\", \"battle\", \"battles\"\n",
    "                \"invasion\", \"invasions\", \"revolution\", \"revolutions\"]\n",
    "PERSON_FILTER = [\"births\", \"deaths\", \"people\", \"leader\", \"leaders\", \"politicians\", \"politician\",\n",
    "                 \"writer\", \"writers\", \"scientist\", \"scientists\", \"personnel\", \"family\", \"families\",\n",
    "                 \"executive\", \"executives\", \"spy\", \"spies\", \"person\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Explainer Scraping:\n",
    "\n",
    "We define some functions to help scrape wikipedia.\n",
    "\n",
    "We take an initial link, in this case the cold war link. Then we get all references from this link and scrape these. The combined data consists of the initial link and all references from it. We save the paragraph texts, references, title, URL and wikipedias Categories on the bottom of the pages. Meaning that for each page we scrape, we also find all its references in order to build all edges.\n",
    "\n",
    "Our cleanup function makes sure to remove pages that are internal or contributor-focused. A lot of content on wikipedia is information about editing wikipedia itself, which we are not interested in for this work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def hyperlink_cleanup(str_link, language):\n",
    "    link_prefix = f\"https://{language}.wikipedia.org\"\n",
    "    if \"/wiki/\" in str_link:\n",
    "        out = link_prefix + str_link\n",
    "        if '#' in out:\n",
    "            out = out.split('#')[0]\n",
    "        if 'Wikipedia' in out:\n",
    "            return False\n",
    "        if 'Template' in out:\n",
    "            return False\n",
    "        return out\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_content_soup(link_wikipedia):\n",
    "    website = requests.get(link_wikipedia)\n",
    "    content_soup = BeautifulSoup(website.content)\n",
    "    return content_soup\n",
    "\n",
    "def soup_get_title(wiki_content_soup):\n",
    "    head = wiki_content_soup.find(\"h1\", {\"id\": \"firstHeading\"})\n",
    "    return head.text\n",
    "\n",
    "def soup_get_reference_links(wiki_content_soup, language=\"en\"):\n",
    "    p_elements = wiki_content_soup.find_all(\"p\")\n",
    "    links = [a['href'] for p in p_elements for a in p.find_all(\"a\", href=True)]\n",
    "    links = [hyperlink_cleanup(link, language) for link in links]\n",
    "    links = [link for link in links if link != False]\n",
    "    return links\n",
    "\n",
    "def soup_get_category_texts(wiki_content_soup, language=\"en\"):\n",
    "    html_div = wiki_content_soup.find(\"div\", {\"id\": \"mw-normal-catlinks\"})\n",
    "    links_without_categories = html_div.find(\"ul\")\n",
    "    links = links_without_categories.find_all(\"a\")\n",
    "    link_texts = [a.text for a in links]\n",
    "    return link_texts\n",
    "\n",
    "def soup_get_paragraph_texts(wiki_content_soup):\n",
    "    p_elements = wiki_content_soup.find_all(\"p\")\n",
    "    paragraph_texts = [p.text for p in p_elements]\n",
    "    return paragraph_texts\n",
    "\n",
    "def get_all_reference_links(link_wikipedia, language):\n",
    "    website = requests.get(link_wikipedia)\n",
    "    content_soup = BeautifulSoup(website.content)\n",
    "    p_elements = content_soup.find_all(\"p\")\n",
    "    links = [a['href'] for p in p_elements for a in p.find_all(\"a\", href=True)]\n",
    "    links = [hyperlink_cleanup(link, language) for link in links]\n",
    "    links = [link for link in links if link != False]\n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Scrape English version\n",
    "if OPTION_PERFORM_SCRAPE:\n",
    "    links_to_scan = get_all_reference_links(LINK_WIKI_ENGLISH, \"en\") + [LINK_WIKI_ENGLISH]\n",
    "    links_to_scan = sorted(list(set(links_to_scan)))\n",
    "    data = []\n",
    "    for url in tqdm(links_to_scan):\n",
    "        soup = get_content_soup(url)\n",
    "        title = soup_get_title(soup)\n",
    "        list_references = soup_get_reference_links(soup, language=\"en\")\n",
    "        list_paragraph_texts = soup_get_paragraph_texts(soup)\n",
    "        list_category_texts = soup_get_category_texts(soup)\n",
    "        data.append([url, title, list_references, list_paragraph_texts, list_category_texts])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save English version\n",
    "if OPTION_PERFORM_SCRAPE:\n",
    "    COLUMN_NAMES = ['URL', 'TITLE', 'LIST_REFERENCES', 'LIST_PARAGRAPH_TEXTS', \"CATEGORIES\"]\n",
    "    df_wikipedia_english = pd.DataFrame(data, columns=COLUMN_NAMES)\n",
    "    df_wikipedia_english = df_wikipedia_english.set_index('URL')\n",
    "    df_wikipedia_english.to_csv('./data/wiki_english.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Scrape German Version\n",
    "if OPTION_PERFORM_SCRAPE:\n",
    "    links_to_scan = get_all_reference_links(LINK_WIKI_GERMAN, language=\"de\") + [LINK_WIKI_GERMAN]\n",
    "    links_to_scan = sorted(list(set(links_to_scan)))\n",
    "    data = []\n",
    "    for url in tqdm(links_to_scan):\n",
    "        soup = get_content_soup(url)\n",
    "        title = soup_get_title(soup)\n",
    "        list_references = soup_get_reference_links(soup, language=\"de\")\n",
    "        list_paragraph_texts = soup_get_paragraph_texts(soup)\n",
    "        list_category_texts = soup_get_category_texts(soup)\n",
    "        data.append([url, title, list_references, list_paragraph_texts, list_category_texts])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save German Version\n",
    "if OPTION_PERFORM_SCRAPE:\n",
    "    COLUMN_NAMES = ['URL', 'TITLE', 'LIST_REFERENCES', 'LIST_PARAGRAPH_TEXTS', \"CATEGORIES\"]\n",
    "    df_wikipedia_german = pd.DataFrame(data, columns=COLUMN_NAMES)\n",
    "    df_wikipedia_german = df_wikipedia_german.set_index('URL')\n",
    "    df_wikipedia_german.to_csv('./data/wiki_german.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#df_wikipedia_english = pd.read_csv('./data/wiki_english.csv', index_col='URL', converters={'LIST_REFERENCES': literal_eval, 'LIST_PARAGRAPH_TEXTS': literal_eval, \"CATEGORIES\": literal_eval})\n",
    "\n",
    "#df_wikipedia_english['TYPE'] = df_wikipedia_english_fromCSV.apply(lambda x: wiki_util.get_category(x['CATEGORIES'], EVENT_FILTER, PERSON_FILTER), axis=1)\n",
    "\n",
    "df_wikipedia_english = pd.read_csv('./data/wiki_english_with_tokens.csv', index_col='URL', converters={'LIST_REFERENCES': literal_eval, 'LIST_PARAGRAPH_TEXTS': literal_eval, \"CATEGORIES\": literal_eval, \"TOKENS\": literal_eval, \"UNIQUE_TOKENS\": literal_eval})\n",
    "\n",
    "#df_wikipedia_german_fromCSV = pd.read_csv('./data/wiki_german.csv', index_col='URL', converters={'LIST_REFERENCES': literal_eval, 'LIST_PARAGRAPH_TEXTS': literal_eval, \"CATEGORIES\": literal_eval})\n",
    "\n",
    "\n",
    "#df_wikipedia_german_fromCSV['TYPE'] = df_wikipedia_german_fromCSV.apply(lambda x: wiki_util.get_category(x['CATEGORIES'], EVENT_FILTER, PERSON_FILTER), axis=1)\n",
    "\n",
    "# Virker til en vis grÃ¦nse, nogle smÃ¥ting der ikke bliver fanget ordenligt. None filteret virker bedst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Explainer:\n",
    "Here is the function for producing communities using the Louvain Community Detection Algorithm as explained in the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_graph_stats(graph):\n",
    "    def format_print(str1, str2):\n",
    "        print(\"{:>15}\".format(str1), \"{:>15}\".format(str2))\n",
    "\n",
    "    if graph.is_directed():\n",
    "        in_degrees = [x[1] for x in graph.in_degree]\n",
    "        out_degrees = [x[1] for x in graph.out_degree]\n",
    "    else:\n",
    "        degrees = [x[1] for x in graph.degree]\n",
    "\n",
    "    format_print('Statistic', 'Value')\n",
    "    format_print('N Nodes', len(graph.nodes))\n",
    "    format_print('N Edges', len(graph.edges))\n",
    "    if graph.is_directed():\n",
    "        format_print('Max in_degree', max(in_degrees))\n",
    "        format_print('Min in_degree', min(in_degrees))\n",
    "        format_print('Max out_degree', max(out_degrees))\n",
    "        format_print('Min out_degree', min(out_degrees))\n",
    "        format_print('Mean in_degree', \"{: 2.2f}\".format(np.mean(in_degrees)))\n",
    "        format_print('Mean out_degree', \"{: 2.2f}\".format(np.mean(out_degrees)))\n",
    "    else:\n",
    "        format_print('Max degree', max(degrees))\n",
    "        format_print('Min degree', min(degrees))\n",
    "        format_print('Mean degree', \"{: 2.2f}\".format(np.mean(degrees)))\n",
    "\n",
    "def produce_communities(graph, resolution=0.3):\n",
    "    list_communities = nx.community.louvain_communities(graph, seed=1, resolution=resolution)\n",
    "\n",
    "    list_communities.sort(key=len, reverse=True)\n",
    "    #print(len(list_communities))\n",
    "\n",
    "    nice_colors = ['#0fdbff', '#0fdbff', '#ff0fb3', '#5e3582', '#ffe70f', '#1e9648', '#1e6296', '#4a1e96', '#961e6a', '#51888c']\n",
    "\n",
    "    DEFAULT_COLOR = '#8c8c8c'\n",
    "    partition_colors = defaultdict(lambda: DEFAULT_COLOR)\n",
    "\n",
    "    for i in range(len(nice_colors)):\n",
    "        partition_colors[i] = nice_colors[i]\n",
    "\n",
    "    for node in graph.nodes:\n",
    "        for i in range(len(list_communities)):\n",
    "            if node in list_communities[i]:\n",
    "                graph.nodes[node]['color'] = partition_colors[i]\n",
    "                graph.nodes[node]['community'] = i\n",
    "\n",
    "    return graph, len(list_communities), list_communities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Explainer: Building the Edge List\n",
    "\n",
    "When we want to create a graph, we have to create one from an Edge List. We go through the data and build a reference count dict, which has the key (urlFrom, urlTo), with value weight. Defaultdict allows us to assume 0 if no entries yet, making it safe to simply increment the value, instead of checking for None.\n",
    "\n",
    "From this dict we can create a directed graph or undirected by simply summing weights (A, B) and (B, A) for each edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def df_get_url_list(df_wiki):\n",
    "    return list(df_wiki.index)\n",
    "\n",
    "def mask_list(base, to_mask):\n",
    "    res = [o for o in to_mask if o in base]\n",
    "    return res\n",
    "\n",
    "def get_dict_reference_count(df_wiki):\n",
    "    reference_count = defaultdict(lambda:0)\n",
    "\n",
    "    url_list = df_get_url_list(df_wiki)\n",
    "    for url in tqdm(url_list):\n",
    "        references = df_wiki[\"LIST_REFERENCES\"][url]\n",
    "        references = mask_list(url_list, references)\n",
    "\n",
    "        for url_ref in references:\n",
    "            reference_count[(url, url_ref)] += 1\n",
    "\n",
    "    return reference_count\n",
    "\n",
    "def get_edge_list(df_wiki, directed=False):\n",
    "    reference_count = get_dict_reference_count(df_wiki)\n",
    "\n",
    "    edge_list = []\n",
    "    url_list = df_get_url_list(df_wiki)\n",
    "    for a in tqdm(url_list):\n",
    "        for b in url_list:\n",
    "            weight = reference_count[(a,b)]\n",
    "            if not directed:\n",
    "                weight += reference_count[(b,a)]\n",
    "            if weight > 0:\n",
    "                if directed:\n",
    "                    edge_list.append((a, b, weight))\n",
    "                elif not ((a, b, weight) in edge_list or (b, a, weight) in edge_list):\n",
    "                    edge_list.append((a, b, weight))\n",
    "\n",
    "    return edge_list\n",
    "\n",
    "def generate_graph_with_node_attributes(graph, df_wiki):\n",
    "    for node in graph.nodes:\n",
    "        graph.nodes[node]['TYPE'] = df_wiki.TYPE[node]\n",
    "        PARAGRAPH_TEXTS = df_wiki.LIST_PARAGRAPH_TEXTS[node]\n",
    "        graph.nodes[node]['LIST_PARAGRAPH_TEXTS'] = PARAGRAPH_TEXTS\n",
    "        graph.nodes[node]['FLAT_TEXT'] = ' '.join(PARAGRAPH_TEXTS)\n",
    "        graph.nodes[node]['CATEGORIES'] = df_wiki.CATEGORIES[node]\n",
    "        graph.nodes[node]['TITLE'] = df_wiki.TITLE[node]\n",
    "    return graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def threshold_node_degree_undirected(graph, min=0, max=30):\n",
    "    if graph.is_directed():\n",
    "        raise Exception(\"Only use on undirected_graphs\")\n",
    "    nodes = (\n",
    "        node for node, data in graph.nodes(data=True)\n",
    "        if min <= graph.degree[node] <= max\n",
    "    )\n",
    "    return graph.subgraph(nodes)\n",
    "\n",
    "def threshold_node_degree_directed(graph, min=[0, 0], max=[30,30]):\n",
    "    if not graph.is_directed():\n",
    "        raise Exception(\"Only use on directed graphs\")\n",
    "    if len(min) != 2:\n",
    "        raise Exception(\"Min must be array of length 2\")\n",
    "    if len(max) != 2:\n",
    "        raise Exception(\"Max must be array of length 2\")\n",
    "    nodes = (\n",
    "        node for node, data in graph.nodes(data=True)\n",
    "        if min[0] <= graph.in_degree[node] <= max[0]\n",
    "        and min[1] <= graph.out_degree[node] <= max[1]\n",
    "    )\n",
    "    return graph.subgraph(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 859/859 [00:01<00:00, 564.72it/s]\n",
      "100%|ââââââââââ| 859/859 [00:12<00:00, 66.36it/s] \n",
      "100%|ââââââââââ| 859/859 [00:01<00:00, 513.67it/s]\n",
      "100%|ââââââââââ| 859/859 [00:00<00:00, 2173.99it/s]\n"
     ]
    }
   ],
   "source": [
    "en_edge_list = get_edge_list(df_wikipedia_english)\n",
    "en_edge_list_directed = get_edge_list(df_wikipedia_english, directed=True)\n",
    "\n",
    "graph_en_directed = nx.DiGraph()\n",
    "graph_en_directed.add_weighted_edges_from(en_edge_list_directed)\n",
    "graph_en_directed = generate_graph_with_node_attributes(graph_en_directed, df_wikipedia_english)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Explainer: Tokenizer\n",
    "\n",
    "We initially used our own implementation of TF-IDF, but this proved too slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tf_idf_sklearn(documents, max_df=1, min_df=1, max_features=100):\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', stop_words='english',\n",
    "                                 max_features=max_features,\n",
    "                                 max_df=max_df, min_df=min_df)\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    return tfidf_matrix, vectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Graph and Text analysis\n",
    "In the cell below is a class, which we made as a wrapper that makes it simple for us to iterate and experiment with different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_attributes_for_community(graph, community, attribute):\n",
    "    list_attribute = []\n",
    "    for (n, d) in graph.nodes(data=True):\n",
    "        if d['community'] == community:\n",
    "            list_attribute.append(d[attribute])\n",
    "\n",
    "    return list_attribute\n",
    "\n",
    "def get_attributes_for_graph(graph, attribute):\n",
    "    list_attribute = []\n",
    "    for (n, d) in graph.nodes(data=True):\n",
    "        list_attribute.append(d[attribute])\n",
    "\n",
    "    return list_attribute\n",
    "\n",
    "def get_subgraph(graph, attribute, values):\n",
    "    nodes = (\n",
    "        node for node, data in graph.nodes(data=True)\n",
    "        if data.get(attribute) in values\n",
    "    )\n",
    "    return graph.subgraph(nodes)\n",
    "\n",
    "class CSSGraph:\n",
    "    def __init__(self, directed=False):\n",
    "        self.n_communities = 0\n",
    "        self.communities = None\n",
    "        self.directed = directed\n",
    "        self.visualize_config = None\n",
    "        self.graph_corpus = None\n",
    "        self.community_corpus = [] #Maybe not needed\n",
    "\n",
    "        self.tf_idf_df = None\n",
    "\n",
    "        self.community_top_10_tf = {}\n",
    "        self.community_top_10_tf_idf = {}\n",
    "        self.community_top_3_nodes = {}\n",
    "\n",
    "        self.vectorizer = None\n",
    "        self.tf_idf_matrix = None\n",
    "\n",
    "        if directed:\n",
    "            self.graph = nx.DiGraph()\n",
    "        else:\n",
    "            self.graph = nx.Graph()\n",
    "\n",
    "    def add_weighted_edges_from(self, edge_list):\n",
    "        self.graph.add_weighted_edges_from(edge_list)\n",
    "\n",
    "    def populate_node_attributes(self, df):\n",
    "        self.graph = generate_graph_with_node_attributes(self.graph, df)\n",
    "\n",
    "    def print_graph_stats(self):\n",
    "        get_graph_stats(self.graph)\n",
    "\n",
    "    def make_subgraph_with_attribute_values(self, attribute, values):\n",
    "        self.graph = get_subgraph(self.graph, attribute=attribute, values=values)\n",
    "\n",
    "    def filter_nodes_by_degree(self, d_min, d_max):\n",
    "        if self.directed:\n",
    "            self.graph = threshold_node_degree_directed(self.graph, min = d_min, max= d_max)\n",
    "        else:\n",
    "            self.graph = threshold_node_degree_undirected(self.graph, min=d_min, max=d_max)\n",
    "\n",
    "    def embed_communities(self, resolution=1):\n",
    "        self.graph, self.n_communities, self.communities = produce_communities(self.graph, resolution)\n",
    "        self.embed_graph_corpus()\n",
    "        self.embed_community_corpus()\n",
    "\n",
    "    def visualize(self, graph_saving=False):\n",
    "        if not graph_saving:\n",
    "            visualize(self.graph)\n",
    "        elif self.visualize_config is None:\n",
    "            _, self.visualize_config = visualize(self.graph)\n",
    "        else:\n",
    "            visualize(self.graph, config=self.visualize_config)\n",
    "\n",
    "    def embed_graph_corpus(self):\n",
    "        if self.graph_corpus is None:\n",
    "            self.graph_corpus = get_attributes_for_graph(self.graph, 'FLAT_TEXT')\n",
    "\n",
    "    def embed_tf_idf(self, max_df=1, min_df=1, max_features=100):\n",
    "        self.tf_idf_matrix, self.vectorizer = tf_idf_sklearn(self.community_corpus, max_df=max_df, min_df=min_df, max_features=max_features)\n",
    "        self.tf_idf_df = pd.DataFrame(self.tf_idf_matrix.toarray(), columns=self.vectorizer.get_feature_names_out())\n",
    "\n",
    "    def get_top_terms(self, community=None):\n",
    "        if community is None:\n",
    "            return [list(self.tf_idf_df.iloc[c].sort_values(ascending=False)[:10].index) for c in range(self.n_communities)]\n",
    "        else:\n",
    "            return list(self.tf_idf_df.iloc[community].sort_values(ascending=False)[:10].index)\n",
    "\n",
    "    def get_nodes_in_community(self, community):\n",
    "        return [node for node, c in self.graph.nodes(data='community') if c == community]\n",
    "\n",
    "    def get_top_nodes(self, community=0, n_max = 3):\n",
    "        nodes_in_community = self.get_nodes_in_community(community)\n",
    "        degrees = [(node,val) for (node, val) in self.graph.degree() if node in nodes_in_community]\n",
    "        sorted_degree= sorted(degrees, key=lambda x: x[1], reverse=True)[:n_max]\n",
    "        return [node for node, val in sorted_degree]\n",
    "\n",
    "    # Maybe not needed\n",
    "    def embed_community_corpus(self):\n",
    "        if len(self.community_corpus) != self.n_communities:\n",
    "            for n in range(self.n_communities):\n",
    "                self.community_corpus.append(' '.join(get_attributes_for_community(self.graph, n, 'FLAT_TEXT')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Subgraph without communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "graph_en_basic = CSSGraph()\n",
    "graph_en_basic.add_weighted_edges_from(en_edge_list)\n",
    "graph_en_basic.populate_node_attributes(df_wikipedia_english)\n",
    "\n",
    "graph_en_basic.make_subgraph_with_attribute_values('TYPE', ['event'])\n",
    "\n",
    "#graph_en_basic.visualize(graph_saving=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Full Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "graph_en_full = CSSGraph()\n",
    "graph_en_full.add_weighted_edges_from(en_edge_list)\n",
    "graph_en_full.populate_node_attributes(df_wikipedia_english)\n",
    "\n",
    "# Community Analysis\n",
    "graph_en_full.embed_communities(resolution=1)\n",
    "graph_en_full.embed_tf_idf(max_df=0.99, min_df=0, max_features=100)\n",
    "\n",
    "#graph_en_full.visualize(graph_saving=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Sub Graph for Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "graph_en_event = CSSGraph()\n",
    "graph_en_event.add_weighted_edges_from(en_edge_list)\n",
    "graph_en_event.populate_node_attributes(df_wikipedia_english)\n",
    "\n",
    "# Filter Graph\n",
    "graph_en_event.make_subgraph_with_attribute_values('TYPE', ['event'])\n",
    "\n",
    "# Community Analysis\n",
    "graph_en_event.embed_communities(resolution=1)\n",
    "graph_en_event.embed_tf_idf(max_df=0.99, min_df=0, max_features=100)\n",
    "\n",
    "#graph_en_event.visualize(graph_saving=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Subgraph for event with degree threshold (Selected for further analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "graph_en = CSSGraph()\n",
    "graph_en.add_weighted_edges_from(en_edge_list)\n",
    "graph_en.populate_node_attributes(df_wikipedia_english)\n",
    "\n",
    "# Filter Graph\n",
    "graph_en.make_subgraph_with_attribute_values('TYPE', ['event'])\n",
    "graph_en.filter_nodes_by_degree(d_min=2, d_max=40)\n",
    "\n",
    "# Community Analysis\n",
    "graph_en.embed_communities(resolution=1)\n",
    "graph_en.embed_tf_idf(max_df=0.99, min_df=0, max_features=100)\n",
    "\n",
    "#graph_en.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Graph Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Note on modularity\n",
    "\n",
    "We see that the modularity is much higher on the graph filtered by event and then threshold-filtered based on node degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Statistics\n",
    "graph_en.print_graph_stats()\n",
    "graph_en_full.print_graph_stats()\n",
    "print(\"Modularity:\", nx.community.modularity(graph_en.graph, graph_en.communities))\n",
    "print(\"Modularity event:\", nx.community.modularity(graph_en_event.graph, graph_en_event.communities))\n",
    "print(\"Modularity full:\", nx.community.modularity(graph_en_full.graph, graph_en_full.communities))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Assortativity\n",
    "\n",
    "Here we use NetworksX's implementation of Louvains' Community Detection algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Assortativity degree subgraph:\", nx.degree_assortativity_coefficient(graph_en.graph))\n",
    "print(\"Assortativity degree full graph:\", nx.degree_assortativity_coefficient(graph_en_full.graph))\n",
    "print(\"Assortativity type attribute full graph:\", nx.attribute_assortativity_coefficient(graph_en_full.graph, 'TYPE'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Degree Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "G = graph_en.graph\n",
    "\n",
    "if OPTION_SHOW_PLOT:\n",
    "    degrees = [x[1] for x in G.degree]\n",
    "    bins = np.linspace(min(degrees), max(degrees), max(degrees))\n",
    "\n",
    "    mean = np.mean(degrees)\n",
    "    median = np.median(degrees)\n",
    "\n",
    "    hist, edges = np.histogram(degrees, bins=bins)\n",
    "    x = (edges[1:] + edges[:-1])/2\n",
    "    width = bins[1] - bins[0]\n",
    "    fig, axs = plt.subplots(1, figsize=(6, 3))\n",
    "    axs.bar(x, hist, width=width*0.9)\n",
    "\n",
    "    axs.set_xlabel('Degrees')\n",
    "    axs.set_ylabel('Articles')\n",
    "    axs.set_yscale('log')\n",
    "    axs.set_xticks(range(0, 501, 5))\n",
    "    axs.set_yticks([10**0, 10**1, 10**2, max(hist)], labels=[10**0, 10**1, 10**2, max(hist)])\n",
    "    axs.axline((mean, 0), (mean, max(degrees)), color='red', label='mean degrees')\n",
    "    axs.axline((median, 0), (median, max(degrees)), linestyle='--', color='red', label='median degrees')\n",
    "    axs.legend()\n",
    "\n",
    "    fig.tight_layout(pad=0.5)\n",
    "    plt.show()\n",
    "    if OPTION_SAVE_FIG:\n",
    "        fig.savefig(\"./images/degree_distributions_final.png\")\n",
    "\n",
    "G = graph_en_full.graph\n",
    "\n",
    "if OPTION_SHOW_PLOT:\n",
    "    degrees = [x[1] for x in G.degree]\n",
    "    bins = np.linspace(min(degrees), max(degrees), 30)\n",
    "\n",
    "    mean = np.mean(degrees)\n",
    "    median = np.median(degrees)\n",
    "\n",
    "    hist, edges = np.histogram(degrees, bins=bins)\n",
    "    x = (edges[1:] + edges[:-1])/2\n",
    "    width = bins[1] - bins[0]\n",
    "    fig, axs = plt.subplots(1, figsize=(6, 3))\n",
    "    axs.bar(x, hist, width=width*0.9)\n",
    "\n",
    "    axs.set_xlabel('Degrees')\n",
    "    axs.set_ylabel('Articles')\n",
    "    axs.set_yscale('log')\n",
    "    axs.set_xticks(range(0, 501, 40))\n",
    "    axs.set_yticks([10**0, 10**1, 10**2, max(hist)], labels=[10**0, 10**1, 10**2, max(hist)])\n",
    "    axs.axline((mean, 0), (mean, max(degrees)), color='red', label='mean degrees')\n",
    "    axs.axline((median, 0), (median, max(degrees)), linestyle='--', color='red', label='median degrees')\n",
    "    axs.legend()\n",
    "\n",
    "    fig.tight_layout(pad=0.5)\n",
    "    plt.show()\n",
    "    if OPTION_SAVE_FIG:\n",
    "        fig.savefig(\"./images/degree_distributions_final_fullgraph.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get top terms and stats\n",
    "print(\"Number of communities:\", graph_en.n_communities)\n",
    "print(\"Top 10 terms in community 0\")\n",
    "print(graph_en.get_top_terms(0))\n",
    "print(\"Top 3 nodes in community 0\")\n",
    "print(graph_en.get_top_nodes(0))\n",
    "\n",
    "# Get top terms of graph\n",
    "tf_idf_matrix, vectorizer = tf_idf_sklearn(graph_en.graph_corpus, max_df=0.99, min_df=1, max_features=100)\n",
    "importance = np.argsort(np.asarray(tf_idf_matrix.sum(axis=0)).ravel())[::-1]\n",
    "tfidf_feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "print(tfidf_feature_names[importance[:10]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# WordClouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#plt.style.use('classic')\n",
    "fig, axs =  plt.subplots(nrows=3, ncols=3, figsize=(20, 12))\n",
    "plt.subplots_adjust(hspace=0.2)\n",
    "for community, ax in zip(range(graph_en.n_communities), axs.ravel()):\n",
    "    top3 = graph_en.get_top_nodes(community, 3)\n",
    "    top3 = [graph_en.graph.nodes[t]['TITLE'] for t in top3]\n",
    "    text = graph_en.community_corpus[community]\n",
    "    wordcloud = WordCloud().generate(text)\n",
    "    ax.imshow(wordcloud)\n",
    "    ax.set_title(top3, wrap=True)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}